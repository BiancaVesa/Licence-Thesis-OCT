import glob
import pandas as pd
import pickle

with open('data/dataframe.txt', 'rb') as dataframe_file:
    df = pickle.load(dataframe_file)

with open('data/acuity.txt', 'rb') as acuity_dataframe:
    acuity_df = pickle.load(acuity_dataframe)

to_delete = [('5', 'OS'), ('12', 'OD'), ('15', 'OS'), ('15', 'OD'), ('23', 'OS'), ('23', 'OD'), ('31', 'OS'), ('50', 'OS'), ('79', 'OD'), ('94', 'OD')]

def is_removable(patient, eye):
    """
    Checks if the patient's B-scan has to be excluded from the final dataset
    """
    for (p, e) in to_delete:
        if patient == p and eye == e:
            return True

    return False

def get_acuity(patient, visit_date, eye):
    """
    Returns acuity and acuity type from the dataframe that contains all acuity values, corresponding to a certain B-scan
    identified by the triplet (patient, visit_date, eye)
    """
    acuity = acuity_df[(acuity_df['Patient folder'] == patient) & (acuity_df['Visit Date'].str.contains(visit_date[2:])) & (acuity_df['Eye'] == eye)]['Acuity']
    acuity_type = acuity_df[(acuity_df['Patient folder'] == patient) & (acuity_df['Visit Date'].str.contains(visit_date[2:])) & (acuity_df['Eye'] == eye)]['Acuity types']

    if len(acuity) != 0:
        if acuity.iloc[0] != -1:
            return acuity.iloc[0], acuity_type.iloc[0]
        else:
            return None, None
    else:
        return None, None

def get_image_names_for_B_scan(patient, visit_date, eye):
    """
    Returns all image names from the dataframe that contains OCT details, that correspond to a certain B-scan identified by
    the triplet (patient, visit_date, eye)
    """
    if eye == 'OD':
        eye = 'right'
    else:
        eye = 'left'
    return df[(df['Patient folder'] == patient) & (df['Date'] == visit_date) & (df['Eye'] == eye)]['Image name'].tolist()


def get_volumetric_tensor(encoder_contrastive, image_paths_list, image_size):
    """
    Generates a 3D tensor from the values generated by the encoder from images in image_paths_list
    :encoder_contrastive: encoder model
    :image_paths_list: list of length 25 containing all paths to a B-scan of images
    :image_size: dimension of the image that has to be read
    :return: 3D tensor of the corresponding image and the encodings generated by encoder_model
    """
    input_img = keras.Input(shape=(image_size, image_size, image_channels))
    encoder_model = keras.Sequential(
        [
            input_img,
            get_augmenter(**classification_augmentation),
            encoder_contrastive
        ],
        name="encoder",
    )
    encodings = []

    for image_path in image_paths_list:
        image = tf.keras.utils.load_img(image_path, target_size=(image_size, image_size, 3))
        img_array = tf.keras.utils.img_to_array(image)
        img_array = tf.expand_dims(img_array, 0)
        prediction = encoder_model.predict(img_array)
        encodings.append(prediction)

    imgs.append(image_paths_list)
    volumetric_tensor = layers.Concatenate(axis=0)(encodings)
    num_rows, num_cols = volumetric_tensor.get_shape().as_list()
    shapes.append((num_rows, num_cols))

    return volumetric_tensor, encodings

def get_local_paths_from_image_names(local_paths, image_names):
    """
    Returns local paths corresponding to the OCTs that have the names in the image_names list
    """
    group_images = []
    
    for image_name in image_names:
        for local_path in local_paths:
            if image_name in local_path:
                group_images.append(local_path)
                break

    return group_images

tensors_normalized = []
tensors = []
patients_info = []
shapes = []
imgs = []

def generate_volumetric_image(model, images, b_scan_path, image_size, normalize_factor):
    """Converts a 3D tensor generated by get_volumetric_tensor into an image of size 25 x encoder_size
    :model: encoder_model to generate volumetric tensor
    :images: all image paths in dataset
    :b_scan_path: B-scan folder path
    :image_size: dimension of the images in B-scan
    :normalize_factor: normalization factor for tensors
    """

    eye = b_scan_path[-2:]
    visit_date = b_scan_path[-13:-3]
    index_patient_end = b_scan_path.index("Pacient") + 8
    index_visit_start = b_scan_path.index("Vizita") - 1
    index_visit_nr_start = b_scan_path.index("Vizita") + 7
    index_visit_nr_end = b_scan_path.index("-") - 1
    visit_nr = b_scan_path[index_visit_nr_start:index_visit_nr_end]
    patient = b_scan_path[index_patient_end:index_visit_start]
    if not is_removable(patient, eye):
        acuity, acuity_type = get_acuity(patient, visit_date, eye)
        if acuity is not None:
            image_names = get_image_names_for_B_scan(patient, visit_date, eye)
            local_paths = get_local_paths_from_image_names(images, image_names)
            volumetric_tensor, encodings = get_volumetric_tensor(model, local_paths, image_size)
            tensors.append(volumetric_tensor)
            volumetric_tensor = tf.math.divide(volumetric_tensor, normalize_factor)
            tensors_normalized.append(volumetric_tensor)

            patients_info.append((patient, visit_date, eye, acuity, acuity_type))
            tensor_file_name = "Patient_" + patient + "_visit_" + visit_nr + "_" + visit_date + "_eye_" + eye + "_acuity_" + str(acuity) + acuity_type
            # write_tensor_in_dataset(tensor_file_name, volumetric_tensor)
            # write_tensor_in_acuity_folder(tensor_file_name, volumetric_tensor, acuity)


def generate_volumetric_dataset(model, images_dir, image_size, normalize_factor):
    """
    Generates dataset made of volumetric images 
    """
    b_scan_paths = glob.glob(images_dir)
    images = glob.glob(images_dir + '/*.tif')
    for p in b_scan_paths:
        generate_volumetric_image(model, images, p, image_size, normalize_factor)

# def write_info():
#     with open("D:/AN4/Licenta/Out/tensors_128.txt", 'wb') as tensors_file:
#         pickle.dump(tensors, tensors_file)
#
#     with open("D:/AN4/Licenta/Out/tensors_normalized.txt", 'wb') as tensors_normalized_file:
#         pickle.dump(tensors_normalized, tensors_normalized_file)
#
#     with open("D:/AN4/Licenta/Out/patients_info.txt", 'wb') as patients_info_file:
#         pickle.dump(patients_info, patients_info_file)
#
#     with open("D:/AN4/Licenta/Out/shapes.txt", 'wb') as shapes_file:
#         pickle.dump(shapes, shapes_file)
#
#     with open("D:/AN4/Licenta/Out/imgs.txt", 'wb') as imgs_file:
#         pickle.dump(imgs, imgs_file)

def write_tensor_in_dataset(tensor_file, tensor):
    file_name = "D:/AN4/Licenta/Datasets/AMD_encoded/" + tensor_file + ".txt"
    with open(file_name, 'wb') as tensor_file_write:
        pickle.dump(tensor, tensor_file_write)

def write_tensor_in_acuity_folder(tensor_file, tensor, acuity):
    file_name = "D:/AN4/Licenta/Datasets/AMD_encoded_classes_img/" + str(acuity) + "/" + tensor_file + ".jpg"
    with open(file_name, 'wb') as tensor_file_write:
        pickle.dump(tensor, tensor_file_write)
    # plt.imsave(file_name, tensor)
